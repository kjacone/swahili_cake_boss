{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNDq8NbCY7oh"
      },
      "source": [
        "# How to Fine-tuning Gemma for Spoken Language Tasks\n",
        "\n",
        "This notebook demonstrate how to fine tune Gemma for the specific task on replying to email requests that a Korean bakery business might get.\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/kjacone/swahili_cake_boss/blob/main/Gemma/spoken-language-tasks/k-gemma-it/spoken_language_tasks_with_gemma.ipynb#scrollTo=YNDq8NbCY7oh\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzH5Ugf5RlJ"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Select the Colab runtime\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **▾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **L4** or **A100 GPU**.\n",
        "\n",
        "\n",
        "### Gemma setup on Kaggle\n",
        "To complete this tutorial, you'll first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
        "\n",
        "* Get access to Gemma on kaggle.com.\n",
        "* Select a Colab runtime with sufficient resources to run the Gemma 2B model.\n",
        "* Generate and configure a Kaggle username and API key.\n",
        "\n",
        "After you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URMuBzkMVxpU"
      },
      "source": [
        "### Set environemnt variables\n",
        "\n",
        "Set environement variables for ```KAGGLE_USERNAME``` and ```KAGGLE_KEY```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IUOX2hqjV7Ku",
        "outputId": "aff430c3-d273-4f5b-f7a0-e739e58d8851",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
        "\n",
        "# Mounting gDrive for to store artifacts\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXfDwRTQVns2"
      },
      "source": [
        "### Install dependencies\n",
        "\n",
        "Install Keras and KerasNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHs7wpZusEML"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U keras-nlp datasets\n",
        "!pip install -q -U keras\n",
        "\n",
        "# Set the backbend before importing Keras\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "# Avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "\n",
        "# Run at half precision.\n",
        "#keras.config.set_floatx(\"bfloat16\")\n",
        "\n",
        "# Training Configurations\n",
        "token_limit = 512\n",
        "num_data_limit = 100\n",
        "lora_name = \"bankingassistant\"\n",
        "lora_rank = 4\n",
        "lr_value = 1e-4\n",
        "train_epoch = 20\n",
        "model_id = \"gemma2_instruct_2b_en\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load And Translate Data"
      ],
      "metadata": {
        "id": "5Z00jYtPkPZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rWjl1KG2kaUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub"
      ],
      "metadata": {
        "id": "lSuYE4JslNf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "_6BnWO94lE-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Step 1: Download and save the Korean Cake Boss dataset locally\n",
        "# Load the dataset from Hugging Face and save it as JSON lines for offline access\n",
        "ds = load_dataset(\"bebechien/korean_cake_boss\")\n",
        "os.makedirs(\"korean_cake_boss_local\", exist_ok=True)\n",
        "\n",
        "# Save each split as a JSON lines file\n",
        "for split in ds:\n",
        "    with open(f\"korean_cake_boss_local/{split}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in ds[split]:\n",
        "            json.dump(item, f, ensure_ascii=False)\n",
        "            f.write(\"\\n\")  # Newline to separate JSON objects\n",
        "\n",
        "print(\"Dataset downloaded and saved locally.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LkpFVf6XpO4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Load the locally saved dataset\n",
        "def load_local_dataset(filename):\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "    return Dataset.from_dict({\"input\": [entry[\"input\"] for entry in data], \"output\": [entry[\"output\"] for entry in data]})\n",
        "\n",
        "train_ds = load_local_dataset(\"korean_cake_boss_local/train.jsonl\")\n",
        "test_ds = load_local_dataset(\"korean_cake_boss_local/test.jsonl\")\n",
        "local_dataset = DatasetDict({\"train\": train_ds, \"test\": test_ds})\n",
        "\n",
        "# Step 3: Set up the translation models\n",
        "# Load Korean-to-English and English-to-Swahili translation models\n",
        "model_ko_en = \"Helsinki-NLP/opus-mt-ko-en\"\n",
        "tokenizer_ko_en = AutoTokenizer.from_pretrained(model_ko_en)\n",
        "model_ko_en = AutoModelForSeq2SeqLM.from_pretrained(model_ko_en)\n",
        "\n",
        "model_en_sw = \"Helsinki-NLP/opus-mt-en-sw\"\n",
        "tokenizer_en_sw = AutoTokenizer.from_pretrained(model_en_sw)\n",
        "model_en_sw = AutoModelForSeq2SeqLM.from_pretrained(model_en_sw)"
      ],
      "metadata": {
        "id": "uZHP8R41qBnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Translation function\n",
        "def translate_korean_to_swahili(text, tokenizer_ko_en, model_ko_en, tokenizer_en_sw, model_en_sw):\n",
        "    # Korean to English\n",
        "    inputs = tokenizer_ko_en(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model_ko_en.generate(**inputs)\n",
        "    english_text = tokenizer_ko_en.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # English to Swahili\n",
        "    inputs = tokenizer_en_sw(english_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model_en_sw.generate(**inputs)\n",
        "    swahili_text = tokenizer_en_sw.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return swahili_text"
      ],
      "metadata": {
        "id": "wg5khdbFpYVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Translate and save function\n",
        "def translate_and_save(dataset_split, filename, tokenizer_ko_en, model_ko_en, tokenizer_en_sw, model_en_sw):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in dataset_split:\n",
        "            translated_input = translate_korean_to_swahili(item[\"input\"], tokenizer_ko_en, model_ko_en, tokenizer_en_sw, model_en_sw)\n",
        "            translated_output = translate_korean_to_swahili(item[\"output\"], tokenizer_ko_en, model_ko_en, tokenizer_en_sw, model_en_sw)\n",
        "            # Save each translated entry as a dictionary in JSON format\n",
        "            json.dump({\"input\": translated_input, \"output\": translated_output}, f, ensure_ascii=False)\n",
        "            f.write(\"\\n\")  # Newline to separate JSON objects\n",
        "\n",
        "# Translate and save the train and test splits\n",
        "translate_and_save(local_dataset[\"train\"], \"korean_cake_boss_train_sw.json\", tokenizer_ko_en, model_ko_en, tokenizer_en_sw, model_en_sw)\n",
        "translate_and_save(local_dataset[\"test\"], \"korean_cake_boss_test_sw.json\", tokenizer_ko_en, model_ko_en, tokenizer_en_sw, model_en_sw)\n",
        "\n",
        "print(\"Translation completed and saved to korean_cake_boss_train_sw.json and korean_cake_boss_test_sw.json.\")\n"
      ],
      "metadata": {
        "id": "DHJJgctakqTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Optional: Load the translated JSON files back to Dataset objects for further processing or uploading\n",
        "translated_train = load_local_dataset(\"korean_cake_boss_train_sw.json\")\n",
        "translated_test = load_local_dataset(\"korean_cake_boss_test_sw.json\")\n",
        "translated_dataset = DatasetDict({\"train\": translated_train, \"test\": translated_test})\n",
        "\n",
        "# Push the translated dataset to Hugging Face Hub\n",
        "translated_dataset.push_to_hub(\"jacone/swahili_cake_boss\", private=False)\n",
        "\n",
        "print(\"Swahili-translated dataset successfully pushed to Hugging Face Hub.\")"
      ],
      "metadata": {
        "id": "le6MzZaSqLrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUl0t469YfQY"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm4jIEqmYfQY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_nlp\n",
        "\n",
        "import time\n",
        "\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\n",
        "gemma_lm.summary()\n",
        "\n",
        "tick_start = 0\n",
        "\n",
        "def tick():\n",
        "    global tick_start\n",
        "    tick_start = time.time()\n",
        "\n",
        "def tock():\n",
        "    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n",
        "\n",
        "def text_gen(prompt):\n",
        "    tick()\n",
        "    input = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    output = gemma_lm.generate(input, max_length=token_limit)\n",
        "    print(\"\\nGemma output:\")\n",
        "    print(output)\n",
        "    tock()\n",
        "\n",
        "# inference before fine-tuning\n",
        "text_gen(\"Tafadhali andika jibu la barua pepe kwa:\\n\\\"Hujambo, ningependa kuagiza keki moja nambari 3 kwa ajili ya maadhimisho ya harusi yetu. Je, hilo linawezekana?\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T7xe_jzslv4"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiS-KU9osh_N",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_nlp\n",
        "import datasets\n",
        "\n",
        "tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(model_id)\n",
        "\n",
        "# prompt structure\n",
        "# <start_of_turn>user\n",
        "# 다음에 대한 이메일 답장을 작성해줘.\n",
        "# \"{EMAIL CONTENT FROM THE CUSTOMER}\"\n",
        "# <end_of_turn>\n",
        "# <start_of_turn>model\n",
        "# {MODEL ANSWER}<end_of_turn>\n",
        "\n",
        "# input, output\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(\n",
        "    \"bebechien/korean_cake_boss\",\n",
        "    split=\"train\",\n",
        ")\n",
        "print(ds)\n",
        "data = ds.with_format(\"np\", columns=[\"input\", \"output\"], output_all_columns=False)\n",
        "train = []\n",
        "\n",
        "for x in data:\n",
        "  item = f\"<start_of_turn>user\\n다음에 대한 이메일 답장을 작성해줘.\\n\\\"{x['input']}\\\"<end_of_turn>\\n<start_of_turn>model\\n{x['output']}<end_of_turn>\"\n",
        "  length = len(tokenizer(item))\n",
        "  # skip data if the token length is longer than our limit\n",
        "  if length < token_limit:\n",
        "    train.append(item)\n",
        "    if(len(train)>=num_data_limit):\n",
        "      break\n",
        "\n",
        "print(len(train))\n",
        "print(train[0])\n",
        "print(train[1])\n",
        "print(train[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt7Nr6a7tItO"
      },
      "source": [
        "## LoRA Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCucu6oHz53G",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Enable LoRA for the model and set the LoRA rank to 4.\n",
        "gemma_lm.backbone.enable_lora(rank=lora_rank)\n",
        "gemma_lm.summary()\n",
        "\n",
        "# Limit the input sequence length (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = token_limit\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=lr_value,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQQ47kcdpbZ9"
      },
      "source": [
        "Note that enabling LoRA reduces the number of trainable parameters significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26d9npFhAOSp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class CustomCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    model_name = f\"/content/drive/MyDrive/{lora_name}_{lora_rank}_epoch{epoch+1}.lora.h5\"\n",
        "    gemma_lm.backbone.save_lora_weights(model_name)\n",
        "\n",
        "    # Evaluate\n",
        "    text_gen(\"다음에 대한 이메일 답장을 작성해줘.\\n\\\"안녕하세요, 결혼기념일을 위해 3호 케이크 1개를 주문하고 싶은데 가능할까요?\\\"\")\n",
        "\n",
        "history = gemma_lm.fit(train, epochs=train_epoch, batch_size=2, callbacks=[CustomCallback()])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn5-eFiPUkSP"
      },
      "outputs": [],
      "source": [
        "# Example Code for Load LoRA\n",
        "'''\n",
        "train_epoch=17\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\n",
        "# Use the same LoRA rank that you trained\n",
        "gemma_lm.backbone.enable_lora(rank=4)\n",
        "\n",
        "# Load pre-trained LoRA weights\n",
        "gemma_lm.backbone.load_lora_weights(f\"/content/drive/MyDrive/{lora_name}_{lora_rank}_epoch{train_epoch}.lora.h5\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipg1u_wEKTxG"
      },
      "source": [
        "## Try a different sampler\n",
        "\n",
        "The top-K algorithm randomly picks the next token from the tokens of top K probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV5mD_HqKZRF",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "gemma_lm.compile(sampler=\"top_k\")\n",
        "text_gen(\"다음에 대한 이메일 답장을 작성해줘.\\n\\\"안녕하세요, 결혼기념일을 위해 3호 케이크 1개를 주문하고 싶은데 가능할까요?\\\"\")\n",
        "text_gen(\"다음에 대한 이메일 답장을 작성해줘.\\n\\\"안녕하세요, 결혼기념일을 위해 3호 케이크 1개를 주문하고 싶은데 가능할까요?\\\"\")\n",
        "text_gen(\"다음에 대한 이메일 답장을 작성해줘.\\n\\\"안녕하세요, 결혼기념일을 위해 3호 케이크 1개를 주문하고 싶은데 가능할까요?\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m1XaCrlMu3Y"
      },
      "source": [
        "Try a slight different prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC-MLxYWM1HU"
      },
      "outputs": [],
      "source": [
        "text_gen(\"다음에 대한 답장을 작성해줘.\\n\\\"안녕하세요, 결혼기념일을 위해 3호 케이크 1개를 주문하고 싶은데 가능할까요?\\\"\")\n",
        "text_gen(\"아래에 적절한 답장을 써줘.\\n\\\"안녕하세요, 결혼기념일을 위해 3호 케이크 1개를 주문하고 싶은데 가능할까요?\\\"\")\n",
        "text_gen(\"다음에 관한 답장을 써주세요.\\n\\\"안녕하세요, 결혼기념일을 위해 3호 케이크 1개를 주문하고 싶은데 가능할까요?\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UePc572JSUmd"
      },
      "source": [
        "Try a different email inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n5LkXU8Sn6D"
      },
      "outputs": [],
      "source": [
        "text_gen(\"\"\"다음에 대한 이메일 답장을 작성해줘.\n",
        "\"안녕하세요,\n",
        "\n",
        "6월 15일에 있을 행사 답례품으로 쿠키 & 머핀 세트를 대량 주문하고 싶습니다.\n",
        "\n",
        "수량: 50세트\n",
        "구성: 쿠키 2개 + 머핀 1개 (개별 포장)\n",
        "디자인: 심플하고 고급스러운 디자인 (리본 포장 등)\n",
        "문구: \"감사합니다\" 스티커 부착\n",
        "배송 날짜: 6월 14일\n",
        "대량 주문 할인 혜택이 있는지, 있다면 견적과 함께 배송 가능 여부를 알려주시면 감사하겠습니다.\n",
        "\n",
        "감사합니다.\n",
        "\n",
        "박철수 드림\" \"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "spoken_language_tasks_with_gemma.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}